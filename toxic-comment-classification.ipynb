{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<centre><title><h1>Toxic Comment Classification</h1></title></centre>","metadata":{}},{"cell_type":"markdown","source":"<br>\n<center><div class=\"alert alert-block alert-danger\" style=\"margin: 2em;line-height: 1.7em; font-family: Verdana;\"><b style=\"font-size: 18px;\">ðŸ›‘ &nbsp; WARNING:</b><br><br><b>The dataset for this competition contains text that may be considered profane, vulgar, or offensive.</b><br></div></center>","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport gc\nimport random\nimport re\nimport json\nimport pickle\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom path import Path\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom collections import Counter\nfrom wordcloud import WordCloud, STOPWORDS\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy as sp\n\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import SnowballStemmer\nfrom nltk.corpus import stopwords\n\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom plotly import graph_objs as go\n","metadata":{"execution":{"iopub.status.busy":"2022-05-18T10:38:54.6758Z","iopub.execute_input":"2022-05-18T10:38:54.676589Z","iopub.status.idle":"2022-05-18T10:38:59.327761Z","shell.execute_reply.started":"2022-05-18T10:38:54.676468Z","shell.execute_reply":"2022-05-18T10:38:59.326964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"def set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T10:38:59.329733Z","iopub.execute_input":"2022-05-18T10:38:59.330414Z","iopub.status.idle":"2022-05-18T10:38:59.336473Z","shell.execute_reply.started":"2022-05-18T10:38:59.330376Z","shell.execute_reply":"2022-05-18T10:38:59.335664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = \"../input/jigsaw-toxic-comment-classification-challenge/\"\ntrain_data_path = 'train.csv.zip'\ntest_data_path = 'test.csv.zip'\n\ntrain_df = pd.read_csv(path + train_data_path)\nprint(f'shape of train_dataset: {train_df.shape}')\nprint('\\n\\n')\nprint('========== Train Dataset ==========')\nprint('\\n')\ndisplay(train_df.head())","metadata":{"execution":{"iopub.status.busy":"2022-05-18T10:38:59.337751Z","iopub.execute_input":"2022-05-18T10:38:59.338142Z","iopub.status.idle":"2022-05-18T10:39:01.442659Z","shell.execute_reply.started":"2022-05-18T10:38:59.338074Z","shell.execute_reply":"2022-05-18T10:39:01.441697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"temp_df = pd.DataFrame()\ntoxicity = []\ncomment_type = []\ncount = []\nfor col in train_df.columns:\n    if col not in ('id','comment_text'):\n        toxicity.extend([col,col])\n        comment_type.extend(train_df[col].value_counts().keys().tolist())\n        count.extend(train_df[col].value_counts().values)\n\ntemp_df['toxicity'] = toxicity\ntemp_df['comment_type'] = np.array(comment_type,dtype=np.str)\ntemp_df['count'] = count\n\nfig = px.bar(temp_df, x='toxicity', y='count', color='comment_type', title='Value Counts',\n             color_discrete_sequence=['#1616A7','#FB0D0D'],width=500)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T10:39:01.444806Z","iopub.execute_input":"2022-05-18T10:39:01.445164Z","iopub.status.idle":"2022-05-18T10:39:02.521173Z","shell.execute_reply.started":"2022-05-18T10:39:01.445116Z","shell.execute_reply":"2022-05-18T10:39:02.520534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessing(text):\n    text = re.sub(r\" -\", \"\", text)\n    text = re.sub(r\"\\d+:\\d+, \\w+ \\d+, \\d+ \\(\\w+\\)\",\"\",text) # to remove 21:51, January 11, 2016 (UTC)\n    text = re.sub(r\"\\d+.+\\d\",\"\",text)\n    text = re.sub(r\"\\n\",\" \",text)\n    text = re.sub(r\"\\\\\",\"\",text)\n    text = re.sub(r\"\\.\\.\\.|\\.\\.\",\" \",text)\n    text = re.sub(r\":|_|#\",\" \",text)\n    text = re.sub(r\"@|\\||\\(|\\)|!|:|;|\\\"\",\"\",text)\n    text = re.sub(r\"\\.|\\?|,\",\" \", text)\n    return text\n\n\n\ntemp_df = train_df.copy()\n\ntemp_df['comment_text'] = temp_df['comment_text'].apply(preprocessing)\ntemp_df['tokenized_text'] = temp_df['comment_text'].apply(word_tokenize)\n#temp_df['comment_text'] = temp_df['comment_text'].apply(lambda x: ' '.join(x.split()))\n\ntemp_df['len'] = temp_df['comment_text'].apply(len)\n\ntemp_df2 = pd.DataFrame()\n\ntemp_df2['len'] = temp_df['len'].value_counts().keys()\ntemp_df2['count'] = temp_df['len'].value_counts().values\n\ntemp_df2 = temp_df2.sort_values('len')\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=temp_df2['len'].values,y=temp_df2['count'].values,\n                         mode='lines', name='len'))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T10:39:02.522176Z","iopub.execute_input":"2022-05-18T10:39:02.522793Z","iopub.status.idle":"2022-05-18T10:40:57.833484Z","shell.execute_reply.started":"2022-05-18T10:39:02.522759Z","shell.execute_reply":"2022-05-18T10:40:57.832944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"toxic = temp_df[temp_df[\"toxic\"] == 1].reset_index(drop=True)\nsevere_toxic = temp_df[temp_df[\"severe_toxic\"] == 1].reset_index(drop=True)\nobscene = temp_df[temp_df[\"obscene\"] == 1].reset_index(drop=True)\nthreat = temp_df[temp_df[\"threat\"] == 1].reset_index(drop=True)\ninsult = temp_df[temp_df[\"insult\"] == 1].reset_index(drop=True)\nidentity_hate = temp_df[temp_df[\"identity_hate\"] == 1].reset_index(drop=True)\n\nlemma = WordNetLemmatizer()\nstem = SnowballStemmer(\"english\")\ndef most_common(data,n,col):\n    top = Counter([stem.stem(lemma.lemmatize(item)) for sublist in data[col] for item in sublist if item.lower() not in STOPWORDS])\n    temp = pd.DataFrame(top.most_common(20))\n    temp.columns = [\"common_word\",\"count\"]\n    return temp.style.background_gradient(cmap=\"Blues\")\nprint(\"top 20 common words in each toxicity\")\nprint('\\n')\nprint('========== toxic ==========')\ndisplay(most_common(toxic, 20, 'tokenized_text'))\n\nprint('\\n')\nprint('========== severe_toxic ==========')\ndisplay(most_common(severe_toxic, 20, 'tokenized_text'))\n\nprint('\\n')\nprint('========== obscene ==========')\ndisplay(most_common(obscene, 20, 'tokenized_text'))\n\nprint('\\n')\nprint('========== threat ==========')\ndisplay(most_common(threat, 20, 'tokenized_text'))\n\nprint('\\n')\nprint('========== insult ==========')\ndisplay(most_common(insult, 20, 'tokenized_text'))\n\nprint('\\n')\nprint('========== identity_hate ==========')\ndisplay(most_common(identity_hate, 20, 'tokenized_text'))\nprint('\\n')","metadata":{"execution":{"iopub.status.busy":"2022-05-18T10:40:57.834725Z","iopub.execute_input":"2022-05-18T10:40:57.835179Z","iopub.status.idle":"2022-05-18T10:41:23.177656Z","shell.execute_reply.started":"2022-05-18T10:40:57.835117Z","shell.execute_reply":"2022-05-18T10:41:23.176679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def similer_color_func(word=None, font_size=None,\n                       position=None, orientation=None,\n                       font_path=None, random_state=None):\n    h=24\n    s=100\n    l=random_state.randint(30,70)\n    return f'hsl({h}, {s}%, {l}%)'\n\ntext = [stem.stem(lemma.lemmatize(item.lower())) for sublist in toxic['tokenized_text'] for item in sublist if item.lower() not in STOPWORDS]\ntext = ' '.join(text)\n\nmask = np.array(Image.open('../input/images/butterfly-shadow-animal-icon-silhouettes-isolated-dark-black-graphical-white-background-184947266.jpg'))\nwc = WordCloud(mask=mask,\n              background_color='black',\n              max_words=2000,\n              stopwords=STOPWORDS,\n              max_font_size=256,\n              width=mask.shape[1],\n              height=mask.shape[0],\n              collocations=False, \n              font_path=\"../input/font-style/SalmaAlfasans-Light.otf\",\n              color_func=similer_color_func)\n\nwc.generate(text)\nfig = plt.figure(figsize=(15, 15))\nplt.title(\"- Most Common Words within Toxic -\",\n           size=22, weight=\"bold\")\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T11:31:14.401029Z","iopub.execute_input":"2022-05-18T11:31:14.401344Z","iopub.status.idle":"2022-05-18T11:31:30.527325Z","shell.execute_reply.started":"2022-05-18T11:31:14.401311Z","shell.execute_reply":"2022-05-18T11:31:30.526499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def similer_color_func(word=None, font_size=None,\n                       position=None, orientation=None,\n                       font_path=None, random_state=None):\n    h=24\n    s=100\n    l=random_state.randint(30,70)\n    return f'hsl({h}, {s}%, {l}%)'\n\ntext = [stem.stem(lemma.lemmatize(item.lower())) for sublist in severe_toxic['tokenized_text'] for item in sublist if item.lower() not in STOPWORDS]\ntext = ' '.join(text)\n\nmask = np.array(Image.open('../input/images/20854.Jpg'))\nwc = WordCloud(mask=mask,\n              background_color='black',\n              max_words=2000,\n              stopwords=STOPWORDS,\n              max_font_size=256,\n              width=mask.shape[1],\n              height=mask.shape[0],\n              collocations=False,  \n              font_path=\"../input/font-style/SalmaAlfasans-Light.otf\",\n              color_func=similer_color_func)\n\nwc.generate(text)\nfig = plt.figure(figsize=(15, 15))\nplt.title(\"- Most Common Words within Severe toxic -\",\n           size=22, weight=\"bold\")\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T11:34:39.972841Z","iopub.execute_input":"2022-05-18T11:34:39.973523Z","iopub.status.idle":"2022-05-18T11:34:45.871845Z","shell.execute_reply.started":"2022-05-18T11:34:39.973488Z","shell.execute_reply":"2022-05-18T11:34:45.871044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def similer_color_func(word=None, font_size=None,\n                       position=None, orientation=None,\n                       font_path=None, random_state=None):\n    h=24\n    s=100\n    l=random_state.randint(60,100)\n    return f'hsl({h}, {s}%, {l}%)'\n\ntext = [stem.stem(lemma.lemmatize(item.lower())) for sublist in obscene['tokenized_text'] for item in sublist if item.lower() not in STOPWORDS]\ntext = ' '.join(text)\n\nmask = np.array(Image.open('../input/images/istockphoto-858216614-612x612.jpg'))\nwc = WordCloud(mask=mask,\n              background_color='black',\n              max_words=2000,\n              stopwords=STOPWORDS,\n              max_font_size=256,\n              width=mask.shape[1],\n              height=mask.shape[0],\n              collocations=False,  \n              font_path=\"../input/font-style/SalmaAlfasans-Light.otf\",\n              color_func=similer_color_func)\n\nwc.generate(text)\nfig = plt.figure(figsize=(15, 15))\nplt.title(\"- Most Common Words within Obscene -\",\n           size=22, weight=\"bold\")\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T11:33:23.123468Z","iopub.execute_input":"2022-05-18T11:33:23.123872Z","iopub.status.idle":"2022-05-18T11:33:31.705681Z","shell.execute_reply.started":"2022-05-18T11:33:23.123841Z","shell.execute_reply":"2022-05-18T11:33:31.702433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def similer_color_func(word=None, font_size=None,\n                       position=None, orientation=None,\n                       font_path=None, random_state=None):\n    h=24\n    s=100\n    l=random_state.randint(30,70)\n    return f'hsl({h}, {s}%, {l}%)'\n\ntext = [stem.stem(lemma.lemmatize(item.lower())) for sublist in threat['tokenized_text'] for item in sublist if item.lower() not in STOPWORDS]\ntext = ' '.join(text)\n\nmask = np.array(Image.open('../input/images/Picsart_22-05-18_16-33-42-111.jpg'))\nwc = WordCloud(mask=mask,\n              background_color='black',\n              max_words=2000,\n              stopwords=STOPWORDS,\n              max_font_size=256,\n              width=mask.shape[1],\n              height=mask.shape[0],\n              collocations=False,  \n              font_path=\"../input/font-style/SalmaAlfasans-Light.otf\",\n              color_func=similer_color_func)\n\nwc.generate(text)\nfig = plt.figure(figsize=(15, 15))\nplt.title(\"- Most Common Words within Threat -\",\n           size=22, weight=\"bold\")\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T11:35:40.716018Z","iopub.execute_input":"2022-05-18T11:35:40.716346Z","iopub.status.idle":"2022-05-18T11:35:51.235968Z","shell.execute_reply.started":"2022-05-18T11:35:40.716314Z","shell.execute_reply":"2022-05-18T11:35:51.235185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def similer_color_func(word=None, font_size=None,\n                       position=None, orientation=None,\n                       font_path=None, random_state=None):\n    h=24\n    s=100\n    l=random_state.randint(30,70)\n    return f'hsl({h}, {s}%, {l}%)'\n\ntext = [stem.stem(lemma.lemmatize(item.lower())) for sublist in insult['tokenized_text'] for item in sublist if item.lower() not in STOPWORDS]\ntext = ' '.join(text)\n\nmask = np.array(Image.open('../input/images/red-kite-rapture-black-silhouette-cut-out-and-isolated-on-a-white-background-2C4B3E9.jpg'))\nwc = WordCloud(mask=mask,\n              background_color='black',\n              max_words=2000,\n              stopwords=STOPWORDS,\n              max_font_size=256,\n              width=mask.shape[1],\n              height=mask.shape[0],\n              collocations=False,  \n              font_path=\"../input/font-style/SalmaAlfasans-Light.otf\",\n              color_func=similer_color_func)\n\nwc.generate(text)\nfig = plt.figure(figsize=(15, 15))\nplt.title(\"- Most Common Words within Insult -\",\n           size=22, weight=\"bold\")\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T12:21:41.353729Z","iopub.execute_input":"2022-05-18T12:21:41.354046Z","iopub.status.idle":"2022-05-18T12:22:23.612362Z","shell.execute_reply.started":"2022-05-18T12:21:41.35401Z","shell.execute_reply":"2022-05-18T12:22:23.611572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def similer_color_func(word=None, font_size=None,\n                       position=None, orientation=None,\n                       font_path=None, random_state=None):\n    h=24\n    s=100\n    l=random_state.randint(30,70)\n    return f'hsl({h}, {s}%, {l}%)'\n\ntext = [stem.stem(lemma.lemmatize(item.lower())) for sublist in identity_hate['tokenized_text'] for item in sublist if item.lower() not in STOPWORDS]\ntext = ' '.join(text)\n\nmask = np.array(Image.open('../input/images/2d27acd5d288284587e13b0411e6e48a.jpg'))\nwc = WordCloud(mask=mask,\n              background_color='black',\n              max_words=500,\n              stopwords=STOPWORDS,\n              max_font_size=256,\n              width=mask.shape[1],\n              height=mask.shape[0],\n              collocations=False,  \n              font_path=\"../input/font-style/SalmaAlfasans-Light.otf\",\n              color_func=similer_color_func)\n\nwc.generate(text)\nfig = plt.figure(figsize=(15, 15))\nplt.title(\"- Most Common Words within Identity hate -\",\n           size=22, weight=\"bold\")\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T12:45:19.994438Z","iopub.execute_input":"2022-05-18T12:45:19.994729Z","iopub.status.idle":"2022-05-18T12:45:32.942531Z","shell.execute_reply.started":"2022-05-18T12:45:19.9947Z","shell.execute_reply":"2022-05-18T12:45:32.941591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}